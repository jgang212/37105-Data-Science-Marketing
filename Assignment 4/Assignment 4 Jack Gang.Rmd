---
title: "Advertising Effects"
author: "Jack Gang"
date: "2/14/2017"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
header-includes: \usepackage{color}
graphics: yes
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, comment = NA, message = FALSE,
                      fig.width = 10, fig.height = 10, fig.align = "center")
```

## Overview

Our goal is to estimate own and competitive brand-level advertising effects. We combined store-level sales data from the Nielsen RMS scanner data set with DMA-level advertising exposure data from the Nielsen Ad Intel advertising data. We compared estimates based on a within-market strategy that controlled for cross-sectional heterogeneity across markets with a border strategy that exploited the discontinuity in advertising at the common border between two neighboring DMA's. This assignment replicated some results that are part of an ongoing research project with Brad Shapiro that focuses on providing a comprehensive and general overview of advertising effectiveness across a large number of brands.

```{r}
# import packages
library(bit64)
library(data.table)
library(RcppRoll)
library(ggplot2)
library(lfe)
library(stargazer)
library(knitr)
```

## Data

### Brands and product modules

We imported the `brands_DT` table and chose Prilosec in the Antacids category for our analysis:

```{r}
# import brands data
load("/classes/37105/main/Assignment-4/Brands.RData")

# select antacids
selected_module = 8412

# select Prilosec
selected_brand = 621727
```

## Data preparation

We then loaded the store meta-data from `Stores-DMA.RData`. We also loaded the RMS store-level scanner data from `move_8412.RData` and the Nielsen Ad Intel DMA-level TV advertising data from `adv_8412.RData`:

```{r}
# import store data
load("/classes/37105/main/Assignment-4/Stores-DMA.RData")

# import RMS scanner data
load("/classes/37105/main/Assignment-4/move_8412.RData")

# import Nielsen ad data
load("/classes/37105/main/Assignment-4/adv_8412.RData")
```

Both the RMS scanner data and the Ad Intel advertising data include information for the top four brands in the category (product module). To make our analysis computationally more manageable, we did not distinguish among all individual competing brands, but instead we aggregated all competitors into one single brand.

### RMS scanner data (`move`)

For consistency, we renamed the `units` to `quantity` and `promo_percentage` to `promotion` (the promotion variable captures promotional activity as a continuous variable with values between 0 and 1):

```{r}
# rename columns in move
names(move) <- c("brand_code_uc", "store_code_uc", "week_end", "quantity", "price", "promotion")
```

We then created the variable `brand_name` to distinguish between the own and aggregate competitor variables:

```{r}
# create brand_name variable
move[, brand_name := ifelse(brand_code_uc == selected_brand, "own", "comp")]
```

We aggregated the data for each store/week observation, separately for `own` and `comp` data. We took the simple arithmetic `mean` over all competitor brands for prices and promotions:

```{r}
# aggregate data for each store/week observation, separately for own and comp
move = move[,.(total_quantity = sum(quantity), price = mean(price),
                   promotion = mean(promotion)),
                by = .(store_code_uc, week_end, brand_name)]
```

We needed a common key between the RMS scanner data and the Ad Intel advertising data, so we merged the `dma_code` from the `stores` table with the RMS movement data:

```{r}
# extract DMA and store code and only retain unique rows
stores_dma = unique(stores[, .(store_code_uc, dma_code)])

# merge dma_code from stores into move
move = merge(move, stores_dma[, .(store_code_uc, dma_code)], 
             by = c("store_code_uc"))
```

### Ad Intel advertising data (`adv_DT`)

Since the data did not contain observations for all DMA/week combinations during the observation period, we needed to capture that the number of GRP's was 0 for such observations:

```{r}
# extract information for cross join
brands = unique(adv_DT$brand_code_uc)
dma_codes = unique(adv_DT$dma_code)
weeks = seq(from = min(adv_DT$week_end), to = max(adv_DT$week_end), by = "week")

# cross join
setkey(adv_DT, brand_code_uc, dma_code, week_end)
adv_DT = adv_DT[CJ(brands, dma_codes, weeks)]

# replace missing values with 0
adv_DT[is.na(adv_DT)] = 0
```

Similar to what we did with the RMS scanner data, we then created own and competitor names and aggregated data at the DMA/week level. In particular, we aggregated based on the sum of the GRP's, separately for `grp_direct` and `grip_indirect`:

```{r}
# create brand_name variable
adv_DT[, brand_name := ifelse(brand_code_uc == selected_brand, "own", "comp")]

# aggregate data at the DMA/week level, separately for direct and indirect GRP
adv_DT = adv_DT[,.(grp_direct = sum(grp_direct), 
                   grp_indirect = sum(grp_indirect)),
                by = .(dma_code, week_end, brand_name)]
```

We decided to use the sum of the direct and indirect GRP's to create a combined `grp` measure:

```{r}
# combine grp direct and direct
adv_DT[, grp := grp_direct + grp_indirect]
```

### Calculate adstock/goodwill

In order to calculate adstock, first we defined the parameters:

```{r}
# define adstock parameters
N_lags = 52
delta = 0.9
```

Then, we calculated the geometric weights based on the carry-over factor:

```{r}
# calculate geometric weights
geom_weights = cumprod(c(1.0, rep(delta, times = N_lags)))
geom_weights = sort(geom_weights)
tail(geom_weights)
```

Finally, we calculated the adstock variable using the `roll_sum` function:

```{r}
# calculate adstock and add to agg_adv
setkey(adv_DT, brand_name, dma_code, week_end)
adv_DT[, adstock := roll_sum(log(1+grp), n = N_lags+1, weights = geom_weights,
                             normalize = FALSE, align = "right", fill = NA),
       by = .(brand_name, dma_code)]
```

### Merge scanner and advertising data

We then merged the advertising data with the scanner data based on brand name, DMA code, and week:

```{r}
# merge adv data into move
move = merge(move, adv_DT[, .(brand_name, dma_code, week_end, grp_direct, 
                                grp_indirect, grp, adstock)], 
               by = c("brand_name","dma_code","week_end"))
```

### Reshape the data

Next, we reshaped the resulting data from long to wide format. The store code and week variable are the main row identifiers, and we also added the dma_code to the row variables. Quantity, price, promotion, and adstock are the column variables:

```{r}
# reshape data
move = dcast(move, dma_code + store_code_uc + week_end ~ brand_name,
             value.var = c("total_quantity", "price", "promotion", "adstock", "grp"))
```

Since the adstock variable is not defined for the first `N_lags` weeks in the data, we removed these missing values from `move`:

```{r}
# only keep complete cases
move = move[complete.cases(move)]
```

### Time trend

We also created a time trend/index for each month/year combination in the data:

```{r}
# create time trend variable
firstWeek = min(move$week_end)
minYear = year(firstWeek)
startMonth = month(firstWeek)

move[, time_trend := 1+12*(year(week_end) - minYear)+(month(week_end)-startMonth)]
```

## Data inspection

### Time-series of advertising levels

We looked at an example time-series of weekly GRP's for the Lafayette, Louisiana (`dma_code` = 642):

```{r}
# plot time-series of weekly GRP's for Lafayette, LA
ggplot(move[dma_code == 642], aes(week_end, grp_own)) + 
  geom_line() +
  geom_point() +
  scale_x_date("Year", date_labels = "%Y", date_breaks = "1 years", minor_breaks = NULL)

ggplot(move[dma_code == 642], aes(week_end, grp_comp)) + 
  geom_line() +
  geom_point() +
  scale_x_date("Year", date_labels = "%Y", date_breaks = "1 years", minor_breaks = NULL)
```

### Overall advertising variation

Next, we created a new variable at the DMA-level, `normalized_grp`, which is defined as `100*grp/mean(grp)`. This variable captures the percentage deviation of the GRP observations relative to the DMA-level mean of advertising. We plotted a histogram of this `normalized_grp`:

```{r}
# create normalized_grp
move[, normalized_grp_own := 100*grp_own/mean(grp_own)]
move[, normalized_grp_comp := 100*grp_comp/mean(grp_comp)]

# plot histogram of normalized_grp
ggplot(move, aes(normalized_grp_own)) + 
  geom_histogram() + scale_x_continuous("Normalized GRP own", 
                                        limits = c(0, 500)) +
  scale_y_continuous("Count", limits = c(0, 1000000))

ggplot(move, aes(normalized_grp_comp)) + 
  geom_histogram() + scale_x_continuous("Normalized GRP comp", 
                                        limits = c(0, 500)) +
  scale_y_continuous("Count", limits = c(0, 1000000))

```

## Advertising effect estimation

Finally, we estimated the following models for this case:

1. Base specification that used the log of `1+quantity` as output and the log of prices (own and competitor) and promotions as inputs. This is controlled for store and month/year fixed effects.

2. Add the `adstock` (own and competitor) to specification 1.

3. Like specification 2, but not controlling for time fixed effects.

We combined the results for comparison using the `stargazer` package:

```{r}
# own grp
# base specification
fit_base = felm(log(1+total_quantity_own) ~ log(price_comp) + log(price_own) + 
                  promotion_comp + promotion_own | store_code_uc + time_trend,
                data = move)

# add adstock
fit_ad = felm(log(1+total_quantity_own) ~ log(price_comp) + log(price_own) + 
                  promotion_comp + promotion_own + adstock_comp + adstock_own |
                store_code_uc + time_trend, data = move)

# not controlling for time fixed effects
fit_notime = felm(log(1+total_quantity_own) ~ log(price_comp) + log(price_own) +
                    promotion_comp + promotion_own + adstock_comp + adstock_own |
                    store_code_uc, data = move)

# compare using stargazer
stargazer(fit_base, fit_ad, fit_notime, type = "text", 
          column.labels = c("Base", "Adstock", "No Time FE"), 
          dep.var.labels.include = FALSE)

```

```{r}
# comp grp
# base specification
fit_base_comp = felm(log(1+total_quantity_comp) ~ log(price_comp) + 
                       log(price_own) + promotion_comp + promotion_own | 
                       store_code_uc + time_trend, data = move)

# add adstock
fit_ad_comp = felm(log(1+total_quantity_comp) ~ log(price_comp) + 
                     log(price_own) + promotion_comp + promotion_own + 
                     adstock_comp + adstock_own | store_code_uc + time_trend, 
                   data = move)

# not controlling for time fixed effects
fit_notime_comp = felm(log(1+total_quantity_comp) ~ log(price_comp) + 
                    log(price_own) + promotion_comp + promotion_own + 
                    adstock_comp + adstock_own | store_code_uc, data = move)

# compare using stargazer
stargazer(fit_base_comp, fit_ad_comp, fit_notime_comp, type = "text", 
          column.labels = c("Base", "Adstock", "No Time FE"), 
          dep.var.labels.include = FALSE)
```

## Estimation using border strategy

Lastly, we employed the border strategy discussed in class to estimate the advertising effects.

### Merge border names

The `stores` table contains two variables that we used for the border strategy. First, `on_border` indicates if a store is located in a county at a DMA border (`TRUE`) or not (`FALSE`). Second, for all border stores the table contains the `border_name`. Before merging, we converted the `border_name` variable to a factor representation. This saved memory and helped the `lfe` package to create fixed effects:

```{r}
# convert border_name to factor
stores[, border_name := as.factor(border_name)]

# merge border names in move table
move = merge(move, stores[on_border == TRUE, .(store_code_uc, border_name)],
             allow.cartesian = TRUE)
```

For the border strategy, we estimated the advertising effects based on differences in advertising exposure across two counties on one and the other side of a DMA Border. In particular, we allowed for a common time trend in these two adjacent DMA's that controlled for any organically occurring variation in demand that may be correlated with the overall advertising levels.

Using the border strategy, we estimated two more models:

4. Advertising model with both store fixed effects and border/time fixed effects.

5. Model 4 with standard errors that are clustered at the DMA level.

```{r}
# own grp
# add border/time fixed effects
fit_border = felm(log(1+total_quantity_own) ~ log(price_comp) + log(price_own) + 
                  promotion_comp + promotion_own + adstock_comp + adstock_own | 
                    store_code_uc + border_name:time_trend, data = move)

# S.E.'s clustered at the DMA level
fit_border_cluster = felm(log(1+total_quantity_own) ~ log(price_comp) + 
                            log(price_own) + promotion_comp + promotion_own + 
                            adstock_comp + adstock_own | store_code_uc + 
                            border_name:time_trend | 0 | dma_code, data = move)

# compare with previous using stargazer
stargazer(fit_ad, fit_border, fit_border_cluster, type = "text", 
          column.labels = c("Ad", "Border", "Border Cluster"), 
          dep.var.labels.include = FALSE)
```

```{r}
# comp grp
# add border/time fixed effects
fit_border_comp = felm(log(1+total_quantity_comp) ~ log(price_comp) + 
                         log(price_own) + promotion_comp + promotion_own + 
                         adstock_comp + adstock_own | store_code_uc + 
                         border_name:time_trend, data = move)

# S.E.'s clustered at the DMA level
fit_border_cluster_comp = felm(log(1+total_quantity_comp) ~ log(price_comp) + 
                                 log(price_own) + promotion_comp + 
                                 promotion_own + adstock_comp + adstock_own | 
                                 store_code_uc + border_name:time_trend | 0 | 
                                 dma_code, data = move)

# compare with previous using stargazer
stargazer(fit_ad_comp, fit_border_comp, fit_border_cluster_comp, type = "text", 
          column.labels = c("Ad", "Border", "Border Cluster"), 
          dep.var.labels.include = FALSE)
```


